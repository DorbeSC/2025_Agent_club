{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzuZTjoZkt7a"
      },
      "source": [
        "# 0.Assignments\n",
        "\n",
        "👾 這個陽春的聊天機器人需要被優化！<br>\n",
        "若是一個對話串不間斷地持續進行，送進去的訊息量會很多，tokens數量也會跟著增加，會需要花比較多費用(💸💸💸)，也可能使模型的回應雜訊比較多而回應受到干擾，所以我們可以優化短期記憶。<br>\n",
        "另外，我們希望優化使用者體驗，我們可以根據聊天的內容整理出使用者的屬性，並在每一次跟使用者聊天時，都能根據這個使用者的狀況給予客製化的回應，因此我們要加入長期記憶的功能！\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1. 短期記憶優化\n",
        "\n",
        "(1) 🔰 [基本版] 在短期記憶中，將chatbot node送入llm的訊息中加入trim的優化機制 (依據適當的tokens數量決定)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. 加入長期記憶\n",
        "\n",
        "加入長期記憶，讓聊天機器人能夠記住使用者的資訊（名字、偏好語言、興趣），在下一次對話也能針對同個使用者的資訊，給予個人化的回答。\n",
        "\n",
        "(1) 🔰 [基本版]\n",
        "- chatbot node: 在chatbot node中，將該使用者的資訊取出，讓入prompt中讓llm依據使用者的資訊給予個人化的回答\n",
        "\n",
        "- write_memory node: 在每一次生成回答後，將使用者的資訊整理成一段對使用者的描述(使用llm，給予system prompt做指引，自行設計如何整理、需要整理哪些資訊)，將整理完的資訊整理到store (可跨threads存取的地方)。\n",
        "\n",
        "- config: config從原本的短期記憶只有thread_id, 也要加入user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>\n",
        "\n",
        "\n",
        "(2) 👨‍🎓 [進階版]\n",
        "- chatbot node: 可以決定使用者的問題是否需要從長期記憶中取得資訊，以及需要取得什麼資訊\n",
        "- write_memory node: 可以整理成特定格式 (例如：使用with_structured_output，相關概念可以延伸到R3 tool calling內容)。例如：\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- 也可以自行將graph結構調整自己喜歡的(增刪不同node, conditional router, ...)\n",
        "<br>\n",
        "備註：基本版是需要大家完成的，進階版可以自行決定是否挑戰，Enjoy the ride! 😎"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zprt5eyzemnq"
      },
      "source": [
        "# 1.短期記憶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZHRs_NSsfnF"
      },
      "source": [
        "## (1) 基本版\n",
        "🔰 [基本版] 在短期記憶中，將chatbot node送入llm的訊息中加入trim的優化機制 (依據適當的tokens數量決定)\n",
        "\n",
        "note: 可以邊做邊看一下trim設定的效果以及內部運作的機制"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8Ahe-dgr3Qa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -U langgraph langchain_openai==0.3.15 langchain transformers bitsandbytes langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep_VhJl4yKmN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 會需要一點時間\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beAp0_a0yNsP"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")\n",
        "\n",
        "# 包裝成 LangChain 的 llm 物件\n",
        "llm = HuggingFacePipeline(pipeline=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwyMby4dggqz"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.messages.utils import trim_messages\n",
        "\n",
        "MAX_TOKENS = 2048  # 你可以根據模型實際容量調整\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "\n",
        "def chatbot(state: State):\n",
        "    # 取得對話訊息\n",
        "    messages: list[BaseMessage] = state[\"messages\"]\n",
        "\n",
        "    # 修剪訊息，如果 token 超過上限\n",
        "    trimmed_messages = trim_messages(messages, max_tokens=MAX_TOKENS, tokenizer=tokenizer)\n",
        "\n",
        "    # 丟進 llm 做回應\n",
        "    response = llm.invoke(trimmed_messages)\n",
        "\n",
        "    # 回傳新的 state（原訊息 + 回應）\n",
        "    return {\"messages\": messages + [AIMessage(content=response)]}\n",
        "\n",
        "def estimate_tokens(msgs):\n",
        "    total_tokens = 0\n",
        "    for m in msgs:\n",
        "        role = m[\"role\"]\n",
        "        content = m[\"content\"]\n",
        "        tokenized = tokenizer.encode(f\"{role}: {content}\", add_special_tokens=False)\n",
        "        total_tokens += len(tokenized)\n",
        "    return total_tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def chatbot(state: State):\n",
        "  # 💻code below:\n",
        "  # hint: you can use langchain_core trim_messages function to trim your trim_messages, and count_tokens_approximately to count tokens\n",
        "\n",
        "\n",
        "\n",
        "# 建立graph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot) # 在graph裡面加入chatbot的node\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# 加入短期記憶\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfjeu3c4uhzz"
      },
      "outputs": [],
      "source": [
        "# 看一下graph\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ld1Zg3ersQC"
      },
      "outputs": [],
      "source": [
        "def stream_graph_updates(user_input: str, config: dict):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "      if \"chatbot\" in event:\n",
        "        for value in event.values():\n",
        "          print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn6-NIHc0jSG"
      },
      "outputs": [],
      "source": [
        "# 設定對話config (第一次對話)\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}} # thread_id: 對話id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp-MDjLF0ntY"
      },
      "outputs": [],
      "source": [
        "# 開始對話 (可以輸入quit, exit, q，三選一停止對話)\n",
        "while True:\n",
        "  try:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqg436etzLs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2TZ8VqBpuA"
      },
      "source": [
        "# 2.長期記憶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZSFFrWiuE3v"
      },
      "source": [
        "## (1) 基本版\n",
        "🔰 [基本版]\n",
        "- chatbot node: 在chatbot node中，將該使用者的資訊取出，讓入prompt中讓llm依據使用者的資訊給予個人化的回答\n",
        "\n",
        "- write_memory node: 在每一次生成回答後，將使用者的資訊整理成一段對使用者的描述(使用llm，給予system prompt做指引，自行設計如何整理、需要整理哪些資訊)，將整理完的資訊整理到store (可跨threads存取的地方)。\n",
        "\n",
        "- config: config從原本的短期記憶只有thread_id, 也要加入user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VPEkk6s1uZEg"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5czQ-VSKBICQ"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.checkpoint.memory import MemorySaver # within-thread memory\n",
        "from langgraph.store.memory import InMemoryStore # cross-thread store\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "\n",
        "#def chatbot(__________):\n",
        "  # 💻code here:\n",
        "  # TODO:\n",
        "  # 依據user_id取得長期記憶\n",
        "  # 將長期記憶也放進system prompt中，讓llm可以個人化回覆\n",
        "def chatbot(state: State, config: dict):\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # 🚩取得長期記憶\n",
        "    print(\"get user_profile\")\n",
        "    user_profile = store.search(user_id) or \"（尚未有使用者資訊）\"\n",
        "    print(\"end get user_profile\")\n",
        "\n",
        "    # 🚩簡易 trim：只保留最近的 N 則訊息\n",
        "    trimmed_messages = state[\"messages\"][-6:]  # 假設只保留最後 6 則訊息\n",
        "\n",
        "    # 🚩組 prompt：加入 system prompt\n",
        "    messages = [SystemMessage(content=f\"你是一個貼心助理。根據以下使用者資訊給予個人化回覆：\\n{user_profile}\")]\n",
        "    messages.extend(trimmed_messages)\n",
        "\n",
        "    # 🚩呼叫 LLM\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=response.content)]}\n",
        "\n",
        "  #return {\"messages\": [AIMessage(content=response)]}\n",
        "\n",
        "\n",
        "#def write_memory(________):\n",
        "  # 💻code here:\n",
        "  # TODO:\n",
        "  # 將使用者的對話整理成要儲存成長期記憶的資訊，並存入長期記憶\n",
        "\n",
        "def write_memory(state: State, config: dict):\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    history = state[\"messages\"][-2:]  # 最近一次回合 [user, ai]\n",
        "    formatted_history = \"\\n\".join([f\"{msg.type.capitalize()}: {msg.content}\" for msg in history])\n",
        "\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"你是負責整理使用者特徵的助理，請根據對話描述使用者的喜好、需求、個性或關鍵背景資訊。請使用第三人稱書寫，語氣簡潔清楚。\"),\n",
        "        (\"user\", \"{dialogue}\")\n",
        "    ])\n",
        "    summarize_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "    new_summary = summarize_chain.invoke({\"dialogue\": formatted_history})\n",
        "    existing = store.search(user_id)\n",
        "    updated_memory = (existing or \"\") + \"\\n\" + new_summary\n",
        "    store.set(user_id, updated_memory)\n",
        "\n",
        "    return {}\n",
        "\n",
        "# Define the graph\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"chatbot\", chatbot)\n",
        "builder.add_node(\"write_memory\", write_memory)\n",
        "builder.add_edge(START, \"chatbot\")\n",
        "builder.add_edge(\"chatbot\", \"write_memory\")\n",
        "builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "\n",
        "# Compile the graph with the checkpointer fir and store\n",
        "\n",
        "# 💻Code Here\n",
        "# 記得放入短期記憶，長期記憶的store\n",
        "memory = MemorySaver()  # 短期記憶（thread）\n",
        "store = InMemoryStore()  # 長期記憶（跨 thread、依 user_id）\n",
        "\n",
        "graph = builder.compile(checkpointer=memory, store=store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPPiEQpvHKl8"
      },
      "outputs": [],
      "source": [
        "# View\n",
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zjdk4Y1tvXyb"
      },
      "outputs": [],
      "source": [
        "def stream_graph_updates(user_input: str, config: dict):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "        if \"chatbot\" in event:\n",
        "          for value in event.values():\n",
        "              print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GMyA_OCNBIEW"
      },
      "outputs": [],
      "source": [
        "# 使用者A的第一次對話\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\", \"user_id\": \"user_a\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTx7BfHTvVVa"
      },
      "outputs": [],
      "source": [
        "# 開始對話 (可以輸入quit, exit, q，三選一停止對話)\n",
        "while True:\n",
        "  try:\n",
        "    user_input =\n",
        "    input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnwxAcAqvgzE"
      },
      "outputs": [],
      "source": [
        "# 使用者A的第二次對話\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_2\", \"user_id\": \"user_a\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOyjZJ_HvmIk"
      },
      "outputs": [],
      "source": [
        "# 開始對話 (可以輸入quit, exit, q，三選一停止對話)\n",
        "while True:\n",
        "  try:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qIEWoYKwExU"
      },
      "source": [
        "## (2) 進階版\n",
        "\n",
        "👨‍🎓 [進階版]\n",
        "- chatbot node: 可以決定使用者的問題是否需要從長期記憶中取得資訊，以及需要取得什麼資訊\n",
        "- write_memory node: 可以整理成特定格式 (例如：使用with_structured_output，相關概念可以延伸到R3 tool calling內容)。例如：\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- 也可以自行將graph結構調整自己喜歡的(增刪不同node, conditional router, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLcnXZAwHeE"
      },
      "outputs": [],
      "source": [
        "# 💻code here, enjoy the ride 😎\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyJZA50xwZBf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}