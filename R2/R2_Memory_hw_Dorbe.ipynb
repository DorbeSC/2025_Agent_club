{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzuZTjoZkt7a"
      },
      "source": [
        "# 0.Assignments\n",
        "\n",
        "ğŸ‘¾ é€™å€‹é™½æ˜¥çš„èŠå¤©æ©Ÿå™¨äººéœ€è¦è¢«å„ªåŒ–ï¼<br>\n",
        "è‹¥æ˜¯ä¸€å€‹å°è©±ä¸²ä¸é–“æ–·åœ°æŒçºŒé€²è¡Œï¼Œé€é€²å»çš„è¨Šæ¯é‡æœƒå¾ˆå¤šï¼Œtokensæ•¸é‡ä¹Ÿæœƒè·Ÿè‘—å¢åŠ ï¼Œæœƒéœ€è¦èŠ±æ¯”è¼ƒå¤šè²»ç”¨(ğŸ’¸ğŸ’¸ğŸ’¸)ï¼Œä¹Ÿå¯èƒ½ä½¿æ¨¡å‹çš„å›æ‡‰é›œè¨Šæ¯”è¼ƒå¤šè€Œå›æ‡‰å—åˆ°å¹²æ“¾ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥å„ªåŒ–çŸ­æœŸè¨˜æ†¶ã€‚<br>\n",
        "å¦å¤–ï¼Œæˆ‘å€‘å¸Œæœ›å„ªåŒ–ä½¿ç”¨è€…é«”é©—ï¼Œæˆ‘å€‘å¯ä»¥æ ¹æ“šèŠå¤©çš„å…§å®¹æ•´ç†å‡ºä½¿ç”¨è€…çš„å±¬æ€§ï¼Œä¸¦åœ¨æ¯ä¸€æ¬¡è·Ÿä½¿ç”¨è€…èŠå¤©æ™‚ï¼Œéƒ½èƒ½æ ¹æ“šé€™å€‹ä½¿ç”¨è€…çš„ç‹€æ³çµ¦äºˆå®¢è£½åŒ–çš„å›æ‡‰ï¼Œå› æ­¤æˆ‘å€‘è¦åŠ å…¥é•·æœŸè¨˜æ†¶çš„åŠŸèƒ½ï¼\n",
        "\n",
        "<br>\n",
        "\n",
        "### 1. çŸ­æœŸè¨˜æ†¶å„ªåŒ–\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. åŠ å…¥é•·æœŸè¨˜æ†¶\n",
        "\n",
        "åŠ å…¥é•·æœŸè¨˜æ†¶ï¼Œè®“èŠå¤©æ©Ÿå™¨äººèƒ½å¤ è¨˜ä½ä½¿ç”¨è€…çš„è³‡è¨Šï¼ˆåå­—ã€åå¥½èªè¨€ã€èˆˆè¶£ï¼‰ï¼Œåœ¨ä¸‹ä¸€æ¬¡å°è©±ä¹Ÿèƒ½é‡å°åŒå€‹ä½¿ç”¨è€…çš„è³‡è¨Šï¼Œçµ¦äºˆå€‹äººåŒ–çš„å›ç­”ã€‚\n",
        "\n",
        "(1) ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>\n",
        "\n",
        "\n",
        "(2) ğŸ‘¨â€ğŸ“ [é€²éšç‰ˆ]\n",
        "- chatbot node: å¯ä»¥æ±ºå®šä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦éœ€è¦å¾é•·æœŸè¨˜æ†¶ä¸­å–å¾—è³‡è¨Šï¼Œä»¥åŠéœ€è¦å–å¾—ä»€éº¼è³‡è¨Š\n",
        "- write_memory node: å¯ä»¥æ•´ç†æˆç‰¹å®šæ ¼å¼ (ä¾‹å¦‚ï¼šä½¿ç”¨with_structured_outputï¼Œç›¸é—œæ¦‚å¿µå¯ä»¥å»¶ä¼¸åˆ°R3 tool callingå…§å®¹)ã€‚ä¾‹å¦‚ï¼š\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- ä¹Ÿå¯ä»¥è‡ªè¡Œå°‡graphçµæ§‹èª¿æ•´è‡ªå·±å–œæ­¡çš„(å¢åˆªä¸åŒnode, conditional router, ...)\n",
        "<br>\n",
        "å‚™è¨»ï¼šåŸºæœ¬ç‰ˆæ˜¯éœ€è¦å¤§å®¶å®Œæˆçš„ï¼Œé€²éšç‰ˆå¯ä»¥è‡ªè¡Œæ±ºå®šæ˜¯å¦æŒ‘æˆ°ï¼ŒEnjoy the ride! ğŸ˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zprt5eyzemnq"
      },
      "source": [
        "# 1.çŸ­æœŸè¨˜æ†¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZHRs_NSsfnF"
      },
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ] åœ¨çŸ­æœŸè¨˜æ†¶ä¸­ï¼Œå°‡chatbot nodeé€å…¥llmçš„è¨Šæ¯ä¸­åŠ å…¥trimçš„å„ªåŒ–æ©Ÿåˆ¶ (ä¾æ“šé©ç•¶çš„tokensæ•¸é‡æ±ºå®š)\n",
        "\n",
        "note: å¯ä»¥é‚Šåšé‚Šçœ‹ä¸€ä¸‹trimè¨­å®šçš„æ•ˆæœä»¥åŠå…§éƒ¨é‹ä½œçš„æ©Ÿåˆ¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8Ahe-dgr3Qa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -U langgraph langchain_openai==0.3.15 langchain transformers bitsandbytes langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep_VhJl4yKmN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# æœƒéœ€è¦ä¸€é»æ™‚é–“\n",
        "# ä½¿ç”¨ 4-bit é‡åŒ–æ¨¡å‹\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# è¼‰å…¥ tokenizer èˆ‡ 4-bit æ¨¡å‹\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beAp0_a0yNsP"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # åƒ…è¿”å›ç”Ÿæˆçš„å›æ‡‰å…§å®¹\n",
        ")\n",
        "\n",
        "# åŒ…è£æˆ LangChain çš„ llm ç‰©ä»¶\n",
        "llm = HuggingFacePipeline(pipeline=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwyMby4dggqz"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.messages.utils import trim_messages\n",
        "\n",
        "MAX_TOKENS = 2048  # ä½ å¯ä»¥æ ¹æ“šæ¨¡å‹å¯¦éš›å®¹é‡èª¿æ•´\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "\n",
        "def chatbot(state: State):\n",
        "    # å–å¾—å°è©±è¨Šæ¯\n",
        "    messages: list[BaseMessage] = state[\"messages\"]\n",
        "\n",
        "    # ä¿®å‰ªè¨Šæ¯ï¼Œå¦‚æœ token è¶…éä¸Šé™\n",
        "    trimmed_messages = trim_messages(messages, max_tokens=MAX_TOKENS, tokenizer=tokenizer)\n",
        "\n",
        "    # ä¸Ÿé€² llm åšå›æ‡‰\n",
        "    response = llm.invoke(trimmed_messages)\n",
        "\n",
        "    # å›å‚³æ–°çš„ stateï¼ˆåŸè¨Šæ¯ + å›æ‡‰ï¼‰\n",
        "    return {\"messages\": messages + [AIMessage(content=response)]}\n",
        "\n",
        "def estimate_tokens(msgs):\n",
        "    total_tokens = 0\n",
        "    for m in msgs:\n",
        "        role = m[\"role\"]\n",
        "        content = m[\"content\"]\n",
        "        tokenized = tokenizer.encode(f\"{role}: {content}\", add_special_tokens=False)\n",
        "        total_tokens += len(tokenized)\n",
        "    return total_tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def chatbot(state: State):\n",
        "  # ğŸ’»code below:\n",
        "  # hint: you can use langchain_core trim_messages function to trim your trim_messages, and count_tokens_approximately to count tokens\n",
        "\n",
        "\n",
        "\n",
        "# å»ºç«‹graph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot) # åœ¨graphè£¡é¢åŠ å…¥chatbotçš„node\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# åŠ å…¥çŸ­æœŸè¨˜æ†¶\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfjeu3c4uhzz"
      },
      "outputs": [],
      "source": [
        "# çœ‹ä¸€ä¸‹graph\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ld1Zg3ersQC"
      },
      "outputs": [],
      "source": [
        "def stream_graph_updates(user_input: str, config: dict):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "      if \"chatbot\" in event:\n",
        "        for value in event.values():\n",
        "          print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn6-NIHc0jSG"
      },
      "outputs": [],
      "source": [
        "# è¨­å®šå°è©±config (ç¬¬ä¸€æ¬¡å°è©±)\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}} # thread_id: å°è©±id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp-MDjLF0ntY"
      },
      "outputs": [],
      "source": [
        "# é–‹å§‹å°è©± (å¯ä»¥è¼¸å…¥quit, exit, qï¼Œä¸‰é¸ä¸€åœæ­¢å°è©±)\n",
        "while True:\n",
        "  try:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqg436etzLs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2TZ8VqBpuA"
      },
      "source": [
        "# 2.é•·æœŸè¨˜æ†¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZSFFrWiuE3v"
      },
      "source": [
        "## (1) åŸºæœ¬ç‰ˆ\n",
        "ğŸ”° [åŸºæœ¬ç‰ˆ]\n",
        "- chatbot node: åœ¨chatbot nodeä¸­ï¼Œå°‡è©²ä½¿ç”¨è€…çš„è³‡è¨Šå–å‡ºï¼Œè®“å…¥promptä¸­è®“llmä¾æ“šä½¿ç”¨è€…çš„è³‡è¨Šçµ¦äºˆå€‹äººåŒ–çš„å›ç­”\n",
        "\n",
        "- write_memory node: åœ¨æ¯ä¸€æ¬¡ç”Ÿæˆå›ç­”å¾Œï¼Œå°‡ä½¿ç”¨è€…çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå°ä½¿ç”¨è€…çš„æè¿°(ä½¿ç”¨llmï¼Œçµ¦äºˆsystem promptåšæŒ‡å¼•ï¼Œè‡ªè¡Œè¨­è¨ˆå¦‚ä½•æ•´ç†ã€éœ€è¦æ•´ç†å“ªäº›è³‡è¨Š)ï¼Œå°‡æ•´ç†å®Œçš„è³‡è¨Šæ•´ç†åˆ°store (å¯è·¨threadså­˜å–çš„åœ°æ–¹)ã€‚\n",
        "\n",
        "- config: configå¾åŸæœ¬çš„çŸ­æœŸè¨˜æ†¶åªæœ‰thread_id, ä¹Ÿè¦åŠ å…¥user_id\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H4Y0WplOi6R4Eo06Ac2JA_9TbZa2YaRD\" width=\"100\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VPEkk6s1uZEg"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5czQ-VSKBICQ"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.checkpoint.memory import MemorySaver # within-thread memory\n",
        "from langgraph.store.memory import InMemoryStore # cross-thread store\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "\n",
        "#def chatbot(__________):\n",
        "  # ğŸ’»code here:\n",
        "  # TODO:\n",
        "  # ä¾æ“šuser_idå–å¾—é•·æœŸè¨˜æ†¶\n",
        "  # å°‡é•·æœŸè¨˜æ†¶ä¹Ÿæ”¾é€²system promptä¸­ï¼Œè®“llmå¯ä»¥å€‹äººåŒ–å›è¦†\n",
        "def chatbot(state: State, config: dict):\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # ğŸš©å–å¾—é•·æœŸè¨˜æ†¶\n",
        "    print(\"get user_profile\")\n",
        "    user_profile = store.search(user_id) or \"ï¼ˆå°šæœªæœ‰ä½¿ç”¨è€…è³‡è¨Šï¼‰\"\n",
        "    print(\"end get user_profile\")\n",
        "\n",
        "    # ğŸš©ç°¡æ˜“ trimï¼šåªä¿ç•™æœ€è¿‘çš„ N å‰‡è¨Šæ¯\n",
        "    trimmed_messages = state[\"messages\"][-6:]  # å‡è¨­åªä¿ç•™æœ€å¾Œ 6 å‰‡è¨Šæ¯\n",
        "\n",
        "    # ğŸš©çµ„ promptï¼šåŠ å…¥ system prompt\n",
        "    messages = [SystemMessage(content=f\"ä½ æ˜¯ä¸€å€‹è²¼å¿ƒåŠ©ç†ã€‚æ ¹æ“šä»¥ä¸‹ä½¿ç”¨è€…è³‡è¨Šçµ¦äºˆå€‹äººåŒ–å›è¦†ï¼š\\n{user_profile}\")]\n",
        "    messages.extend(trimmed_messages)\n",
        "\n",
        "    # ğŸš©å‘¼å« LLM\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=response.content)]}\n",
        "\n",
        "  #return {\"messages\": [AIMessage(content=response)]}\n",
        "\n",
        "\n",
        "#def write_memory(________):\n",
        "  # ğŸ’»code here:\n",
        "  # TODO:\n",
        "  # å°‡ä½¿ç”¨è€…çš„å°è©±æ•´ç†æˆè¦å„²å­˜æˆé•·æœŸè¨˜æ†¶çš„è³‡è¨Šï¼Œä¸¦å­˜å…¥é•·æœŸè¨˜æ†¶\n",
        "\n",
        "def write_memory(state: State, config: dict):\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    history = state[\"messages\"][-2:]  # æœ€è¿‘ä¸€æ¬¡å›åˆ [user, ai]\n",
        "    formatted_history = \"\\n\".join([f\"{msg.type.capitalize()}: {msg.content}\" for msg in history])\n",
        "\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"ä½ æ˜¯è² è²¬æ•´ç†ä½¿ç”¨è€…ç‰¹å¾µçš„åŠ©ç†ï¼Œè«‹æ ¹æ“šå°è©±æè¿°ä½¿ç”¨è€…çš„å–œå¥½ã€éœ€æ±‚ã€å€‹æ€§æˆ–é—œéµèƒŒæ™¯è³‡è¨Šã€‚è«‹ä½¿ç”¨ç¬¬ä¸‰äººç¨±æ›¸å¯«ï¼Œèªæ°£ç°¡æ½”æ¸…æ¥šã€‚\"),\n",
        "        (\"user\", \"{dialogue}\")\n",
        "    ])\n",
        "    summarize_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "    new_summary = summarize_chain.invoke({\"dialogue\": formatted_history})\n",
        "    existing = store.search(user_id)\n",
        "    updated_memory = (existing or \"\") + \"\\n\" + new_summary\n",
        "    store.set(user_id, updated_memory)\n",
        "\n",
        "    return {}\n",
        "\n",
        "# Define the graph\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"chatbot\", chatbot)\n",
        "builder.add_node(\"write_memory\", write_memory)\n",
        "builder.add_edge(START, \"chatbot\")\n",
        "builder.add_edge(\"chatbot\", \"write_memory\")\n",
        "builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "\n",
        "# Compile the graph with the checkpointer fir and store\n",
        "\n",
        "# ğŸ’»Code Here\n",
        "# è¨˜å¾—æ”¾å…¥çŸ­æœŸè¨˜æ†¶ï¼Œé•·æœŸè¨˜æ†¶çš„store\n",
        "memory = MemorySaver()  # çŸ­æœŸè¨˜æ†¶ï¼ˆthreadï¼‰\n",
        "store = InMemoryStore()  # é•·æœŸè¨˜æ†¶ï¼ˆè·¨ threadã€ä¾ user_idï¼‰\n",
        "\n",
        "graph = builder.compile(checkpointer=memory, store=store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPPiEQpvHKl8"
      },
      "outputs": [],
      "source": [
        "# View\n",
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zjdk4Y1tvXyb"
      },
      "outputs": [],
      "source": [
        "def stream_graph_updates(user_input: str, config: dict):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "        if \"chatbot\" in event:\n",
        "          for value in event.values():\n",
        "              print(\"Assistant:\", value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GMyA_OCNBIEW"
      },
      "outputs": [],
      "source": [
        "# ä½¿ç”¨è€…Açš„ç¬¬ä¸€æ¬¡å°è©±\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_1\", \"user_id\": \"user_a\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTx7BfHTvVVa"
      },
      "outputs": [],
      "source": [
        "# é–‹å§‹å°è©± (å¯ä»¥è¼¸å…¥quit, exit, qï¼Œä¸‰é¸ä¸€åœæ­¢å°è©±)\n",
        "while True:\n",
        "  try:\n",
        "    user_input =\n",
        "    input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnwxAcAqvgzE"
      },
      "outputs": [],
      "source": [
        "# ä½¿ç”¨è€…Açš„ç¬¬äºŒæ¬¡å°è©±\n",
        "config = {\"configurable\": {\"thread_id\": \"conversation_2\", \"user_id\": \"user_a\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOyjZJ_HvmIk"
      },
      "outputs": [],
      "source": [
        "# é–‹å§‹å°è©± (å¯ä»¥è¼¸å…¥quit, exit, qï¼Œä¸‰é¸ä¸€åœæ­¢å°è©±)\n",
        "while True:\n",
        "  try:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "      print(\"Goodbye!\")\n",
        "      break\n",
        "    stream_graph_updates(user_input, config)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qIEWoYKwExU"
      },
      "source": [
        "## (2) é€²éšç‰ˆ\n",
        "\n",
        "ğŸ‘¨â€ğŸ“ [é€²éšç‰ˆ]\n",
        "- chatbot node: å¯ä»¥æ±ºå®šä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦éœ€è¦å¾é•·æœŸè¨˜æ†¶ä¸­å–å¾—è³‡è¨Šï¼Œä»¥åŠéœ€è¦å–å¾—ä»€éº¼è³‡è¨Š\n",
        "- write_memory node: å¯ä»¥æ•´ç†æˆç‰¹å®šæ ¼å¼ (ä¾‹å¦‚ï¼šä½¿ç”¨with_structured_outputï¼Œç›¸é—œæ¦‚å¿µå¯ä»¥å»¶ä¼¸åˆ°R3 tool callingå…§å®¹)ã€‚ä¾‹å¦‚ï¼š\n",
        "```\n",
        "user_profile = {\n",
        "  \"first_name\": \"XXXX\",\n",
        "  \"last_name\": \"OOO\",\n",
        "  \"preferred_lang\": [\"en\", \"zh-tw\"]\n",
        "}\n",
        "```\n",
        "- ä¹Ÿå¯ä»¥è‡ªè¡Œå°‡graphçµæ§‹èª¿æ•´è‡ªå·±å–œæ­¡çš„(å¢åˆªä¸åŒnode, conditional router, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLcnXZAwHeE"
      },
      "outputs": [],
      "source": [
        "# ğŸ’»code here, enjoy the ride ğŸ˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyJZA50xwZBf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}